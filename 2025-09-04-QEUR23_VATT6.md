---
title: QEUR23_VATT6 - Score-CAMを使ってみる
date: 2025-09-04
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "Vibe Coding", "Transformers"]
excerpt: いままでの成果をVibe Codingでレビューする
---

## QEUR23_VATT6 - Score-CAMを使ってみる

## ～ 農業には使える！工業は？ ～

### ・・・ 前回のつづきです ・・・

D先生： “次は、特徴量の可視化をやってみましょう。ConvNextだけをやってみましょう。CoatNetのアテンションマップは、すでに作れますから・・・。まあ、CoatNetの場合は大したパフォーマンスがないので無視してもいいんじゃないですか？“

QEU:FOUNDER ： “おっと！すでにVibe Codingで調べたのですね。”

![imageVATT2-6-1](/2025-09-04-QEUR23_VATT6/imageVATT2-6-1.jpg) 

D先生： “この可視化の結果も、なかなかの出来ですよ。 まあ、ロボット化が視野に入っているご時世なので、そんなことは当たり前なんですがね。じゃあ、ネタ元から行きましょう。“

![imageVATT2-6-2](/2025-09-04-QEUR23_VATT6/imageVATT2-6-2.jpg) 

QEU:FOUNDER  ： “これ（↑）は、D先生、ご自分で調べたんですか？”

### 追加の提案
1. **Score-CAMの検討**:
   - Grad-CAMは勾配ベースであり、深い層で勾配が消失する場合があります。Score-CAMは活性化マップを直接利用し、病気の位置をより正確に捉える可能性があります。

2. **モデルの再トレーニング**:
   - 病気の局所的な特徴を強調するために、データセットに病気の位置を注釈したセグメンテーションマスクを追加し、セグメンテーションと分類のマルチタスク学習を行う。
   - 例: セグメンテーション損失としてDice LossやIoU Lossを追加。

3. **ハイパーパラメータのチューニング**:
   - CAMのリサイズ時にバイキュービック補間（`cv2.INTER_CUBIC`）を使用すると、細かい特徴が保持されやすくなります。

D先生： “高名なGROK先生がアドバイスしてくれたんです。今回のサブテーマです。**「Vibe Codingで、どのようにイノベーションを釣るのか」**・・・。彼らが発出する種々のシグナルに耳を傾ければ、思わぬ結果**（エビで鯛）**が得られるんです。まあね、その「思わぬ成果」といっても、無知な私が知らなかっただけですがね。それでは、プログラムに行きましょう。コードをドン！！”

```python
# ---
# 必要なライブラリのインポート
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import cv2
import numpy as np
from fastai.vision.all import *

# Grad-CAMの実装
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        self.register_hooks()

    def register_hooks(self):
        def forward_hook(module, input, output):
            self.activations = output.detach()

        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0].detach()

        self.target_layer.register_forward_hook(forward_hook)
        if hasattr(self.target_layer, 'register_full_backward_hook'):
            self.target_layer.register_full_backward_hook(backward_hook)
        else:
            self.target_layer.register_backward_hook(backward_hook)

    def generate_cam(self, input_tensor, class_idx=None):
        self.model.eval()
        features = self.model(input_tensor)
        
        if isinstance(features, tuple):
            features = features[0]
            
        if class_idx is None:
            class_idx = features.argmax(dim=1).item()

        self.model.zero_grad()
        one_hot = torch.zeros_like(features)
        one_hot[0][class_idx] = 1
        features.backward(gradient=one_hot, retain_graph=True)

        if self.gradients is None or self.activations is None:
            raise ValueError("Gradients or activations not captured")
            
        pooled_gradients = torch.mean(self.gradients, dim=[2, 3], keepdim=True)  # 空間情報を保持
        cam = self.activations[0] * pooled_gradients
        cam = cam.sum(dim=0)  # チャネルを合計
        cam = F.relu(cam)
        cam = cam.detach().cpu().numpy()
        
        if cam.size == 0 or np.isnan(cam).any() or np.isinf(cam).any():
            raise ValueError("Invalid CAM values")
        
        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)
        
        try:
            cam = cv2.resize(cam.astype(np.float32), (input_tensor.shape[3], input_tensor.shape[2]))
        except Exception as e:
            print(f"Resize failed: {e}")
            target_h, target_w = input_tensor.shape[2], input_tensor.shape[3]
            cam = np.resize(cam, (target_h, target_w))
            
        return cam

# 画像の可視化関数
def visualize_cam(img_tensor, cam, title="Grad-CAM"):
    if len(img_tensor.shape) == 4:
        img = img_tensor[0].permute(1, 2, 0).cpu().numpy()
    else:
        img = img_tensor.permute(1, 2, 0).cpu().numpy()
    
    img_min, img_max = img.min(), img.max()
    if img_max > img_min:
        img = (img - img_min) / (img_max - img_min)
    else:
        img = np.zeros_like(img)

    if cam is not None and cam.size > 0:
        cam = np.clip(cam, 0, 1)
        cam_vis = np.uint8(255 * cam)
        cam_vis = cv2.applyColorMap(cam_vis, cv2.COLORMAP_JET)
        cam_vis = cv2.cvtColor(cam_vis, cv2.COLOR_BGR2RGB)

        overlay = np.float32(img) + np.float32(cam_vis) * 0.4
        overlay = np.clip(overlay, 0, 1)

        plt.figure(figsize=(6, 6))
        plt.imshow(overlay)
        plt.title(title)
        plt.axis("off")
        plt.show()
    else:
        plt.figure(figsize=(6, 6))
        plt.imshow(img)
        plt.title(f"{title} - No CAM available")
        plt.axis("off")
        plt.show()

# 実行コード
# 1. モデルとデータの準備
xb, yb = dls.one_batch()  # 1バッチ取得
xb = xb.cuda()
learn = learn_convnext
print(f"Using model: {type(learn.model)}")

```

D先生：  “上記のモデル(learn)は、前回に学習したConvNextを使用しました。これは、前回説明したように、とても運のよいことに、かなり良いパフォーマンスを持っています。”

![imageVATT2-6-3](/2025-09-04-QEUR23_VATT6/imageVATT2-6-3.jpg) 

D先生： “じゃあ、（晒しを）続けましょう。 “

```python
# 2. Grad-CAMの対象層を指定（ステージ2の最後のブロック）
try:
    target_layer = learn.model[0].model.stages[2].blocks[-1].conv_dw
    print("Using ConvNeXt stage[2] last block as target layer")
except Exception as e:
    print(f"Error selecting target layer: {e}")
    target_layer = find_last_conv_layer(learn.model)
    if target_layer is None:
        raise ValueError("Could not find convolutional layer")

print(f"Target layer: {type(target_layer)}")

# 3. 4枚の画像に対して上位3ラベルのGrad-CAMを生成
grad_cam = GradCAM(learn.model, target_layer)
num_images = 4
for i in range(min(num_images, xb.shape[0])):
    img_tensor = xb[i:i+1]
    with torch.no_grad():
        preds = learn.model(img_tensor)
        if isinstance(preds, tuple):
            preds = preds[0]
        probs = torch.softmax(preds, dim=1)[0]
        top3_indices = probs.argsort(descending=True)[:3]
        top3_probs = probs[top3_indices]
    
    print(f"\nImage {i+1}:")
    print(f"True class: {yb[i].item()}")
    for j, (class_idx, prob) in enumerate(zip(top3_indices, top3_probs)):
        cam = grad_cam.generate_cam(img_tensor, class_idx=class_idx.item())
        visualize_cam(img_tensor, cam, title=f"Image {i+1} - Class {class_idx} (Prob: {prob:.3f})")

```

QEU:FOUNDER  ： “分類クラス毎に画像が出てくるようですね。たしか、全部で10個のクラスがあります。もちろんnormal(正常)も含まれます。その他は、全部病気のモードです。
”

![imageVATT2-6-4](/2025-09-04-QEUR23_VATT6/imageVATT2-6-4.jpg) 

D先生： “SPOT-CAM出力の結果の見方は、GRAD-CAM と同じです。予測の確率が高い3つのクラスを取り上げて、そのクラスを選択した根拠の部分にカラーを付けています。左端は原画像です。“

![imageVATT2-6-5](/2025-09-04-QEUR23_VATT6/imageVATT2-6-5.jpg) 

QEU:FOUNDER  ： “これによれば、画像の当てはめは悪くはないです。他も見てみましょう。”

![imageVATT2-6-6](/2025-09-04-QEUR23_VATT6/imageVATT2-6-6.jpg) 

QEU:FOUNDER ： “もうちょっと画像を拡大しましょう。”

![imageVATT2-6-7](/2025-09-04-QEUR23_VATT6/imageVATT2-6-7.jpg) 

D先生: “いいでしょう？”

QEU:FOUNDER ： “さすがに、D先生が前回の終わりに「鳴り物を鳴らした」だけはあります（笑）。これが、工業的な製品の異常検出に同じパフォーマンスが出るのかは未知数ですね。”

D先生：“これは、やってみるしかないですね。”

![imageVATT2-6-8](/2025-09-04-QEUR23_VATT6/imageVATT2-6-8.jpg) 

QEU:FOUNDER  ： “（端子検査を）やってくれる？”

D先生：“やりましょう！Vibe Codingが・・・（笑）。”


## ～ まとめ ～

QEU:FOUNDER（設定年齢67歳） ： “さすがに、小生もAI時代の流れの速さについていけないです。かつて、T大卒業生の**就職希望NO1の業界**が危なくなるんだからね。さて、C部長のところは、何か変わったことはありますか？”

![imageVATT2-6-9](/2025-09-04-QEUR23_VATT6/imageVATT2-6-9.jpg) 

C部長 : “地味にモノを作っている会社なので、ほとんど変わりません。今回のイノベーションではサービス業界あたりが大きく変わるんでしょうね。さすがに、あのNANO-BANANAにはたまげました。今まで人手をかけていた広告業界が全く変わってきそうですから・・・。”

![imageVATT2-6-10](/2025-09-04-QEUR23_VATT6/imageVATT2-6-10.jpg) 

QEU:FOUNDER ： “なんといっても、自分のところ**「だけ」**は、変わって欲しくはないですね。”

![imageVATT2-6-11](/2025-09-04-QEUR23_VATT6/imageVATT2-6-11.jpg) 

C部長（すでに50代も後半に） : “ボクも取り合えず、先輩方を見習い、なるべく首をひっこめて逃げ切りたいと思っています。”
