---
title: QEUR23_VATT1 - Vibe Codingでファインチューニングする
date: 2025-08-29
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "Vibe Coding", "Transformers"]
excerpt: いままでの成果をVibe Codingでレビューする
---

## QEUR23_VATT1 - Vibe Codingでファインチューニングする

## ～ Vibe Codingを使うと、本当に手間がかかりません ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “それにしても、変な結果なんですよ。画像のど真ん中にネコちゃんがいるのに、認識してくれないんです。代わりに、トイレとか、洗濯機だって・・・（笑）。”

![imageVATT2-2-1](/2025-08-29-QEUR23_VATT2/imageVATT2-2-1.jpg) 

D先生 ： “一番の問題は、アテンションが猫の顔の上にないことです。このAIは、真ん中にいる猫のことに気が付いていないのでしょう。つまり、このモデルには、根本的に欠陥があるんでしょうね。Baseモデルですから、ファインチューニングが必要です。”

QEU:FOUNDER ： “今回は、Vibe Codingして解決しましょう。はっきり言って、今回の例題は基本中の基本に当たりますから、文章が書ければ大丈夫です。さて、これは、あくまで事例としてのVibe Codingの入力文です。”

```python
以下のプログラムを改造し、以下のデータセットを使ってVitモデルをファインチューニングをしたい。
(質問事項)
- "google/vit-base-patch16-224"は、どのようなモデルですか？使用している技術、階層、パラメータ数を教えてください。
- Oxford-IIIT Pet Datasetについて教えてください。画像のサイズ、枚数、ラベルなどを教えてください。
(要求事項)
- Oxford-IIIT Pet Dataset相当のデータセットは、datasetsなどのパッケージに標準装備されていませんか？もしも、相当するようなデータセットがimport可能であればそれを流用したい。
- プログラムには以下のようなデータ拡張の機能を追加してください。
- 以下のGPUを使用する予定です。リソースがこのスペックを超えないように注意してください。
Pythonプログラムを生成してください。
使用予定のGPU"""
Using device: cuda
GPU Name: Quadro P5000
VRAM: 17.06 GB

"""
データセット"""
**Oxford-IIIT Pet Dataset**
   - 37カテゴリ（犬と猫）、7,349枚の画像

Oxford-IIIT Pet Dataset相当のデータセットは、datasetsなどのパッケージに標準装備されていませんか？
もしも、相当するようなデータセットがあるのであればそれを流用したい。私の要求事項は、数種類の猫の品種が分類にあることです。
Python

"""
プログラム"""
# ---
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from transformers import ViTForImageClassification, ViTFeatureExtractor, Trainer, TrainingAr-guments
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# デバイスの設定
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# 1. モデルのロード
model_name = "google/vit-base-patch16-224"
model = ViTForImageClassification.from_pretrained(model_name, num_labels=10, ig-nore_mismatched_sizes=True)  # クラスの数（例：Animal-10なら10）
model.to(device)

# 2. 画像の前処理
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)

# データセットの準備
def load_dataset(dataset_path, split="train"):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # ViTの入力サイズにリサイズ
        transforms.ToTensor(),
        transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
    ])
    dataset = ImageFolder(root=dataset_path, transform=transform)
    return dataset

# 3. データローダーの作成
train_dataset = load_dataset("path/to/train_dataset")  # トレーニングデータのパス
val_dataset = load_dataset("path/to/val_dataset")      # 検証データのパス

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

# 4. ファインチューニングの設定
training_args = TrainingArguments(
    output_dir="./vit_finetuned",           # モデルの保存先
    num_train_epochs=3,                     # エポック数
    per_device_train_batch_size=16,         # トレーニングバッチサイズ
    per_device_eval_batch_size=16,          # 検証バッチサイズ
    evaluation_strategy="epoch",            # 各エポックで検証
    save_strategy="epoch",                  # 各エポックでモデル保存
    logging_dir="./logs",                   # ログ保存先
    logging_steps=10,                       # ログ頻度
    learning_rate=2e-5,                     # 学習率
    load_best_model_at_end=True,            # 最良モデルをロード
    metric_for_best_model="accuracy",       # 最良モデルの基準
)

# 5. 評価メトリクスの定義
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="weighted")
    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

# 6. Trainerの設定
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# 7. ファインチューニングの実行
trainer.train()

# 8. モデルの保存
trainer.save_model("./vit_cat_finetuned_model")
feature_extractor.save_pretrained("./vit_cat_finetuned_model")

"""
データ拡張"""

# 精度向上のため、データ拡張（例：ランダム回転、フリップ）を`transforms`に追加可能：

  transform = transforms.Compose([
      transforms.RandomHorizontalFlip(),
      transforms.RandomRotation(10),
      transforms.Resize((224, 224)),
      transforms.ToTensor(),
      transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
  ])
"""

```

D先生 ： “この質問で生成されたプログラムを晒さないんですか？”

QEU:FOUNDER ： “無意味でしょ？基本は、自分よりも「はるかに賢く、知識のある」AIがやってくれるんだから・・・。定評のあるAnthoropicのコード生成器だったら、はるかに良いモノが出来ると思います。ただし、この文には質問があるので、それだけを見てみましょう。”

### 質問事項への回答

### "google/vit-base-patch16-224" モデルの詳細

- **概要**:  
 `google/vit-base-patch16-224` は、Vision Transformer (ViT) モデルで、画像認識のために設計されたトランスフォーマーエンコーダベースのアーキテクチャです。画像を16x16ピクセルのパッチに分割し、これをトークンとして処理します。ImageNet-21k（1400万画像、21,843クラス）で事前学習され、ImageNet 2012（100万画像、1000クラス）でファインチューニングされています。

- **使用技術**:  
 - **トランスフォーマーエンコーダ**: 自然言語処理で使用されるトランスフォーマーアーキテクチャを画像処理に適用。画像をパッチに分割し、線形埋め込みを行った後、位置エンコーディングを追加してトランスフォーマーに入力します。  
 - **パッチベース処理**: 画像を固定サイズ（16x16）のパッチに分割し、シーケンスとして処理。これにより、長距離依存関係やコンテキスト情報を捉えます。
 - **[CLS] トークン**: 画像全体の表現として使用され、分類タスクに利用されます。

- **階層（レイヤー）**:  
 - トランスフォーマーエンコーダの層数: **12層**  
 - 各層には、マルチヘッド自己注意機構（12個の注意ヘッド）とフィードフォワードネットワーク（FFN）が含まれます。

- **パラメータ数**:  
 - 約 **8600万パラメータ**（ViT-Baseモデルの標準的なパラメータ数）。これは、隠れ層サイズ（768）、中間層サイズ（3072）、およびその他の構成に基づいています。

- **入力仕様**:  
 - 画像サイズ: 224x224ピクセル  
 - パッチサイズ: 16x16ピクセル  
 - チャンネル数: 3（RGB）  
 - 画像は正規化（平均: [0.5, 0.5, 0.5]、標準偏差: [0.5, 0.5, 0.5]）されます。

### Oxford-IIIT Pet Dataset の詳細

- **概要**:  
 Oxford-IIIT Pet Dataset は、37カテゴリのペット（犬25種、猫12種）の画像データセットで、画像分類やオブジェクト検出、セグメンテーションタスクに使用されます。画像にはスケール、ポーズ、照明の大きなバリエーションがあります。

- **画像の詳細**:  
 - **枚数**: 合計約7,349枚（各クラス約200枚）。  
 - **サイズ**: 画像サイズは一貫しておらず、さまざまな解像度で提供されます。ただし、ViTモデルでは通常224x224にリサイズして使用します。  
 - **ラベル**: 37クラス（犬と猫の品種）。各画像には以下のアノテーションが付与されています：  
   - 種別（犬または猫）  
   - 品種名（例: Abyssinian, Bengal, Boxer, Pug など）  
   - 頭部のバウンディングボックス（トレーニングデータにのみ提供）  
   - ピクセルレベルの前景-背景セグメンテーション（トリマップ）。

- **データセットの構造**:  
 - トレーニングセットとテストセットに分割されています。トレーニングセットには頭部のバウンディングボックスが含まれ、テストセットでは空です。 
 - データセットサイズ: 約800MB（画像とアノテーションを含む）。

- **猫の品種**:  
 データセットには12種類の猫の品種（例: Abyssinian, Bengal, Birman, Egyptian Mau, Maine Coon など）が含まれます。これはご要望の「数種類の猫の品種が分類にあること」を満たします。

3. **Oxford-IIIT Pet Dataset 相当のデータセット**

- **標準パッケージでの利用可能性**:  
 はい、Oxford-IIIT Pet Dataset は `datasets` パッケージ（Hugging Face）や `tensorflow_datasets`（TensorFlow Datasets）で標準装備されています。Hugging Face の `datasets` ライブラリを使用すると、簡単にデータセットをロードできます。 
 - 例: `datasets.load_dataset("oxford_iiit_pet")` で直接ロード可能。  
 - このデータセットには、画像、品種ラベル、種別（犬/猫）、セグメンテーションマスク、頭部のバウンディングボックスが含まれます。  
 - 猫の品種分類を含むため、ご要望に適合します。

C部長： “このViTモデルは、8000万もあるのか、そして、レイヤが12あると・・・。それにしても、学習するデータセットは、8000枚もないのですか？こんなに少ない情報で学習できるのですか？足らないのではないのですか？”

QEU:FOUNDER ： “C部長がそれほどの問題意識があるのであれば、自分でAIに質問してください。”


**（現在、C部長が自分で質問中）**

C部長： “さっきのプログラムは、**すべてのViTモデルのパラメタを、あの少ない情報でファインチューニングをします。AI様に、階層を限定して学習させるように言われました！！**”

QEU:FOUNDER ： “それでは、C部長のプログラムを動かしましょう。まずは、学習過程の確認です。5エポックだけにしました。”

![imageVATT2-2-2](/2025-08-29-QEUR23_VATT2/imageVATT2-2-2.jpg) 

D先生： “プログラムを見ると、**第10層と11層を学習対象にした**ようです。95％ぐらいの精度がでてくるんですね。”

QEU:FOUNDER ： “気を付けなければいけないのは、学習データが圧倒的に少ない場合には、過学習する可能性があることです。そういう場合には、学習時の成績がいいのですが、実際の使用時のパフォーマンスが悪くなります。予測の結果を2種類見てみましょう。”

**（事例）**

![imageVATT2-2-3](/2025-08-29-QEUR23_VATT2/imageVATT2-2-3.jpg) 

**（混同行列）**

![imageVATT2-2-4](/2025-08-29-QEUR23_VATT2/imageVATT2-2-4.jpg) 

D先生： “なかなかに良い成績です。ベンガルとエジプシャンマウだけが混乱していますね。まあ、これに関しては情報が足らないのでしょう。さて、次は推論（アテンションマップ）に行きましょう。”

QEU:FOUNDER ： “これも、C部長にやってもらいましょう。ファインチューニングは、10階層、11階層しかやっていないことに注意しましょう。”

```python
以下のプログラムは、任意のサイズの画像データを入力してVitモデルで判別し、アテンションマップを出力します。
現行の判別結果は数字であり、文字列のラベルではありません。以下のラベルのリストを使って、名前を出力してください。
また、従来のアテンションマップは全体的な階層の情報であり、個別の改造であることが求められます。
階層8以降の個別のアテンションマップを生成してください。
Pythonプログラムを生成してください。
使用予定のGPU"""
Using device: cuda
GPU Name: Quadro P5000
VRAM: 17.06 GB

"""
出力"""
予測結果: 4 - LABEL_4
結果を保存: attention_result.png

"""
ラベルのリスト"""
- Abyssinian
- American Bulldog
- American Pit Bull Terrier
- Basset Hound
- Beagle
- Bengal
- Birman
- Bombay
- Boxer
- British Shorthair
- Chihuahua
- Egyptian Mau

"""
プログラム"""
# 以下、省略
"""
```

C部長： “こんな風に質問をつくりました。”

D先生： “それでは、いきなり結果を見てみましょう。”

![imageVATT2-2-5](/2025-08-29-QEUR23_VATT2/imageVATT2-2-5.jpg) 

QEU:FOUNDER ： “学習したレイアのアテンション・マップを見ると、しっかりと**AIのアテンション（注意）が判別のポイントに行っている**ことがわかります。”

D先生： “それにしても、プログラムを晒していないのは新鮮ですね。”

QEU:FOUNDER ： “基本的に、このスタイルを続けたいですね。”


## ～ まとめ ～

### ・・・ AIは偉大ですわ ・・・

QEU:FOUNDER ： “GROKも進化したようです。もともと、我々は大したノウハウを使ってないので、大して影響がありませんがね。それにしても、GROKは速いので良い。”

![imageVATT2-2-6](/2025-08-29-QEUR23_VATT2/imageVATT2-2-6.jpg) 

D先生： “一部に、**「AIを使ってコーディングするとバカになる」**という話がありますが、どうなんですか？”

![imageVATT2-2-7](/2025-08-29-QEUR23_VATT2/imageVATT2-2-7.jpg) 

C部長 : “「自身のスキルを使わなくなる傾向」って・・・。僕には、スキルがありませんから・・・（笑）。”

QEU:FOUNDER ： “AIを使うとバカになるっていうのは、そうなのかもね。・・・でも、**バカには使いこなせないです**。今回の件で、以下の用語(Term)を知らないとどうなったと思う？”

- **アテンションマップ（レイヤー毎）**
- **ファインチューニング（レイヤーを限定した学習）**
- **過学習（モデルが大きく、学習データが小さい場合）**

C部長 : “手も足も出ないでしょうね。”

[![MOVIE1](http://img.youtube.com/vi/sgD0UKYQC6Y/0.jpg)](http://www.youtube.com/watch?v=sgD0UKYQC6Y "ALL in AI: 為什麼你要立即開始Vibe Coding?")

QEU:FOUNDER ： “それにしても、我々はトライ・エラーでこれらの知識を見につけたが、これから先はどうなるのかねえ・・・。今後は、特に若者は、昔のようにのんびりした時間を与えられないから・・・。ともあれ、**「なにかをやりたい（目的）」を持っていれば、何とかなると思うよ。**きっと・・・。”

