---
title: QEUR23_VATT5 - FastaiでCONVNEXTモデルを使ってみる
date: 2025-09-03
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "Vibe Coding", "Transformers"]
excerpt: いままでの成果をVibe Codingでレビューする
---

## QEUR23_VATT5 - FastaiでCONVNEXTモデルを使ってみる

## ～ Vibe Codingをイノベーションに如何に使うか？ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “ともあれ、現時点で発見された一番の問題点は、このシステム（Vit-COATNET）は、入力画像を大きくできないので局部に発生する異常の検出に対応できないことです。”

![imageVATT2-5-1](/2025-09-03-QEUR23_VATT5/imageVATT2-5-1.jpg) 

D先生： “新しいシステムは、農業問題（↑）のようなテーマに対応できること。せっかく画像サイズの制約の緩やかなCNNを使っているのだから、より大きな画像にも対応できることが必要ですね。”

QEU:FOUNDER ： “ここで、データセットを**農業関係**にしましょう。”

![imageVATT2-5-2](/2025-09-03-QEUR23_VATT5/imageVATT2-5-2.jpg) 

QEU:FOUNDER ： “懐かしいでしょ？**KaggleでFastaiです**。Kaggleには、農業関連のデータセットが多いんです。まずは、D先生も黙ってPythonプログラムを見ておいて下さい。どのようなテーマであるかを紹介しますので・・・。”

```python
# ---
%matplotlib inline
from fastkaggle import *
from fastai.vision.all import *
import matplotlib.pyplot as plt

# データセットのセットアップ
comp = 'paddy-disease-classification'
path = setup_comp(comp, install='fastai "timm>=0.6.2.dev0"')
print("Dataset path contents:", path.ls())

# リサイズ画像の保存先
trn_path = Path('small')
print("Resized images path contents:", trn_path.ls())

# 画像を192x256pxにリサイズ
resize_images(path/"train_images", dest=trn_path, max_size=256, recurse=True)
print("Resized images path contents after resizing:", trn_path.ls())

# データローダーの作成
dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42, item_tfms=Resize((256,192)))

# 1. データセットのラベルを表示
print("\nDataset labels:")
for i, label in enumerate(dls.vocab):
    print(f"Label {i}: {label}")

# 2. 各ラベルに対してランダムに3枚の画像を表示
def show_images_per_label(dls, n_images=3):
    # ラベルのリスト
    labels = dls.vocab
    # 図の設定
    for label_idx, label in enumerate(labels):
        print(f"\nShowing {n_images} images for label {label_idx}: {label}")
        # 該当ラベルの画像を取得
        label_path = trn_path/label
        if label_path.exists():
            # 画像ファイルのリストを取得
            image_files = get_image_files(label_path)
            # ランダムにn_images枚を選択
            random_images = random.sample(image_files, min(n_images, len(image_files)))
            
            # 画像を表示
            fig, axes = plt.subplots(1, n_images, figsize=(n_images*4, 4))
            if n_images == 1:
                axes = [axes]  # 単一のaxesをリストに変換
            for i, img_path in enumerate(random_images):
                img = PILImage.create(img_path)
                axes[i].imshow(img)
                axes[i].set_title(f"Label {label_idx}: {label}")
                axes[i].axis('off')
            plt.show()
        else:
            print(f"No images found for label {label}")

# 各ラベルごとの画像表示
show_images_per_label(dls, n_images=3)

```

D先生： “この写真は・・・。どうやら、今回のテーマは**「主食(の植物)」**のようです。もしも、コンピューターが病気を検出できるのであれば、大変に便利ですね。”

**(NO1)**

![imageVATT2-5-3](/2025-09-03-QEUR23_VATT5/imageVATT2-5-3.jpg) 

**(NO2)**

![imageVATT2-5-4](/2025-09-03-QEUR23_VATT5/imageVATT2-5-4.jpg) 

**(NO3)**

![imageVATT2-5-5](/2025-09-03-QEUR23_VATT5/imageVATT2-5-5.jpg) 

**(NO4)**

![imageVATT2-5-6](/2025-09-03-QEUR23_VATT5/imageVATT2-5-6.jpg) 

**(NO5)**

![imageVATT2-5-7](/2025-09-03-QEUR23_VATT5/imageVATT2-5-7.jpg) 

QEU:FOUNDER ： “このデータセットの良いところは、**画像のサイズが大きい**ことなんですよ。まあ、ちょっと不満はあるが、最初は、これでやってみたいと思います。あとは、先生に任せました。つづきをどうぞ・・・（笑）。”

D先生： “じゃあ、不肖ながらVibe Coding担当の私がやらせていただきます。プログラムをドン！！ここでは、Fastaiならではの関数を使っています。このライブラリの適用によって、ずいぶんコードがシンプルになっています。”

```python
######################################
# トレーニング関数
######################################
import torch

def train(arch, item, batch, epochs=5, lr=0.01, bs=64):
    # データローダーの定義
    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42, 
                                       item_tfms=item, batch_tfms=batch, 
                                       bs=bs)
    # モデルの作成
    learn = vision_learner(dls, arch, metrics=error_rate, path='.').to_fp16()

    # 利用可能なGPUの数を取得
    num_gpus = torch.cuda.device_count()
    print(f"Using {num_gpus} GPUs for {arch}")

    # GPUが複数ある場合、DataParallelを使用
    if num_gpus > 1:
        learn.model = torch.nn.DataParallel(learn.model)

    # モデルのトレーニング
    learn.fine_tune(epochs, lr)
    return learn

def find_lr(arch, item, batch, bs=64):
    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, seed=42, 
                                       item_tfms=item, batch_tfms=batch, 
                                       bs=bs)
    learn = vision_learner(dls, arch, metrics=error_rate, path='.').to_fp16()
    return learn.lr_find(suggest_funcs=(valley, slide))

######################################
# RESNETを使う
######################################
arch = 'resnet26'
print(f"\nFinding learning rate for {arch}")
find_lr(arch, item=Resize(192, method='squish'), 
        batch=aug_transforms(size=128, min_scale=0.75), bs=256)

learn_resnet = train(arch, item=Resize(192, method='squish'), 
                     batch=aug_transforms(size=128, min_scale=0.75), 
                     bs=256)

```

D先生：“まずは、簡単なRESNETのトレーニングをして、学習モデルの性能を調べてみましょう。”

![imageVATT2-5-8](/2025-09-03-QEUR23_VATT5/imageVATT2-5-8.jpg) 

QEU:FOUNDER ： “この結果を見て、「やっぱりな・・・」と思いました。このデータセットの病気は「異常の一種」なので、判別すべき特性は「猫とトラの区別」とは違うモノなんです。だから、**エラーが13％**も出てきました。”

D先生：“RESNETって、かなり**古典的なモデル**ですよね。新しいモデルを試してみましょう。**CONVNEXT**というんですが・・・。”

**ResNet26とConvNeXtは、画像判別モデルとして広く使われる畳み込みニューラルネットワーク（CNN）ですが、設計思想や構造、性能、リソース消費において明確な違いがあります。以下に、これらの差異を詳細に解説します。**

---

### 1. **構造の違い**

### **ResNet26**
- **概要**: ResNet（Residual Network）は、2015年にMicrosoft Researchによって提案されたモデルで、残差接続（Skip Connection）を導入することで深いネットワークの学習を可能にしました。ResNet26は、26層の比較的浅いバリエーションです。
- **構造の特徴**:
  - **残差ブロック**: 入力と出力の差分（残差）を学習する残差ブロックを積み重ねる。ResNet26では、基本的な残差ブロック（2層の畳み込み）を使用。
  - **層構成**: 通常、ResNet26は畳み込み層、Batch Normalization（BN）、ReLU活性化関数を組み合わせたブロックで構成される。深い層（例: ResNet-50以上）ではボトルネック構造を採用するが、ResNet26では基本ブロックが主。
  - **ショートカット接続**: 勾配消失問題を軽減し、深いネットワークでも安定した学習を可能にする。
  - **PreActivation**: 一部のResNetでは、BNとReLUを畳み込みの前に配置するPreActivation構成を採用し、学習効率を向上。
- **設計思想**: シンプルで拡張性が高く、勾配消失問題を解決することに重点を置いた汎用的なCNN。

### **ConvNeXt**
- **概要**: ConvNeXtは2022年に提案されたモデルで、ResNetをベースにVision Transformer（ViT）の設計要素を取り入れ、現代的なCNNとして再設計されたもの。
- **構造の特徴**:
  - **モダンな残差ブロック**: ResNetの残差ブロックを改良。1層目が7x7のDepthWise Convolu-tion、2・3層目がPointWise Convolutionで構成される。
  - **LayerNorm**: BatchNormの代わりにLayerNormを採用し、安定性とスケーラビリティを向上。
  - **活性化関数**: ReLUの代わりにGELUを採用し、非線形性を強化。
  - **マクロデザイン**: Swin Transformerのような階層的構造を採用し、ステージごとに特徴マップのサイズを縮小。
  - **パッチ化**: 入力画像をパッチに分割する処理（4x4の畳み込み）を導入し、ViTの影響を受けている。
- **設計思想**: ResNetのシンプルさと効率性を維持しつつ、Vision Transformerの優れたスケーラビリティや性能を取り入れる。

---

### 2. **性能の違い**

### **ResNet26**
- **精度**: ResNet26は、ImageNetデータセットで高い精度を達成するが、層数が浅いため、ResNet-50やResNet-152に比べると精度は劣る。例えば、ILSVRC 2015でResNet全体はTop-5エラー率3.57%を達成したが、ResNet26はこれよりやや低い性能。[](https://pystyle.info/pytorch-resnet/)
- **タスク適性**: 画像分類、物体検出、セグメンテーションなど幅広いタスクに適用可能だが、最新モデルと比較するとやや時代遅れ。
- **拡張性**: 層を増やすことで性能を向上できるが、計算コストも増加する。

### **ConvNeXt**
- **精度**: ConvNeXtは、ImageNetでVision Transformer（例: Swin Transformer）と同等以上の精度を達成。たとえば、ConvNeXt-TinyはImageNetで約82%のTop-1精度を記録し、ResNet-50（約76%）を上回る。
- **タスク適性**: 画像分類だけでなく、物体検出やセマンティックセグメンテーションでも優れた性能を発揮。Transformer系モデルと競合可能。
- **拡張性**: 層数や幅を増やしたバリエーション（ConvNeXt-Tiny, Small, Base, Largeなど）があり、スケーラビリティが非常に高い。

---

### 3. **リソース消費の違い**

### **ResNet26**
- **パラメータ数**: ResNet26は比較的軽量で、パラメータ数は約1,000万程度（ResNet-50の約2,500万と比較して少ない）。
- **計算コスト**: FLOPs（浮動小数点演算数）は浅い層数のため低めで、軽量なデバイスでも動作可能。ただし、最新モデルと比べると効率性は劣る。
- **メモリ使用量**: BatchNormを多用するため、メモリ消費はそこそこ多いが、ConvNeXtに比べると軽量。

#### **ConvNeXt**
- **パラメータ数**: モデルサイズにより異なるが、ConvNeXt-Tinyで約2,900万、ConvNeXt-Baseで約8,800万と、ResNet26より多い。
- **計算コスト**: DepthWise ConvolutionやLayerNormの採用により、計算効率は高いが、ResNet26よりやや重い。FLOPsはモデルサイズに応じて増加。
- **メモリ使用量**: LayerNormはBatchNormよりメモリ効率が良く、Transformer風の設計により大規模データセットでの学習が効率的。

QEU:FOUNDER ： “画像判別用モデルって、もうViTで決まりだと思っていました。まだ、開発が続いていたのですね。”

D先生： “なにしろ、この人（↓）がいる会社ですから・・・。 “

![imageVATT2-5-9](/2025-09-03-QEUR23_VATT5/imageVATT2-5-9.jpg) 

QEU:FOUNDER  ： “・・・（笑）。それでは、プログラムと結果に行きましょう。D先生、お願いします。”

```python
######################################
# CONVNEXTを使う
######################################
# ---
arch = "coatnet_0_rw_224"
print(f"\nFinding learning rate for {arch}")
find_lr(arch, item=Resize(224), 
        batch=aug_transforms(size=224, min_scale=0.75), bs=64)

learn_coatnet = train(arch, item=Resize(224),
                      batch=aug_transforms(size=224, min_scale=0.75), 
                      bs=64, epochs=5, lr=0.01)

```

D先生： “結果は、以下のようになっています。さすがに、我らがヒーロー（↑）の作品です。“

![imageVATT2-5-10](/2025-09-03-QEUR23_VATT5/imageVATT2-5-10.jpg) 

QEU:FOUNDER  ： “すごい性能ですね。そうすると、ちょっと疑問があります。CoatNetは、どれぐらいなんでしょうか？ViTを使っているんだから、もっと性能がいいのではないですか？”

**ConvNeXtとCoAtNetは、どちらも画像認識タスク向けの最先端の深層学習モデルですが、設計思想や技術的特徴が異なります。以下に、要求された項目に基づいて比較を行います。**

### 1. 基本的な比較

| **項目**             | **ConvNeXt**                                                                 | **CoAtNet**                                                                 |
|---------------------|------------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| **開発元**          | Meta AI (FAIR)                                                               | Google Research                                                             |
| **技術**            | 純粋な畳み込みベース（CNN）。Vision Transformer (ViT) の設計を取り入れつつ、Self-Attentionを排除。 | 畳み込み（CNN）とSelf-Attention（Transformer）を融合したハイブリッドモデル。     |
| **パラメータ数**    | ConvNeXt-T: 約28M、ConvNeXt-S: 約50M、ConvNeXt-B: 約89M、ConvNeXt-L: 約198M | CoAtNet-0: 約25M、CoAtNet-1: 約42M、CoAtNet-2: 約75M、CoAtNet-3: 約168M |
| **レイヤー数**      | モデルサイズによるが、例えばConvNeXt-Bは深さ56（ステージごとに3, 3, 27, 3）。 | CoAtNet-3は深さ約40（畳み込みとトランスフォーマーブロックの組み合わせ）。       |
| **アーキテクチャ**  | モダンなCNN設計（パッチ化、大きなカーネル、LayerNorm、GELUなど）。          | 畳み込み（MBConv）とSelf-Attentionブロックを段階的に統合。                   |

**解説**：
- **ConvNeXt**は、Vision Transformerの優れたスケーラビリティやグローバルな受容野を参考にしつつ、畳み込みのみでこれを再現。ResNetをベースに改良を重ね、モダンなCNNの進化形。
- **CoAtNet**は、畳み込みの局所的特徴抽出とトランスフォーマーのグローバルな文脈理解を組み合わせたハイブリッドアプローチ。EfficientNetのMBConvブロックとトランスフォーマーブロックを統合。

### 2. 性能、用途

| **項目**             | **ConvNeXt**                                                                 | **CoAtNet**                                                                 |
|---------------------|------------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| **性能**            | ImageNet-1Kでトップクラスの精度（例：ConvNeXt-Lで87.8% top-1）。ViTと同等かそれ以上の性能。 | ImageNet-1Kで高い精度（例：CoAtNet-3で87.3% top-1）。大規模データではさらに優位。 |
| **計算効率**        | トランスフォーマーより軽量で、GPU/TPUでの推論が高速。                     | ハイブリッド設計のため、計算コストはConvNeXtよりやや高いが、EfficientNet由来の効率性を持つ。 |
| **用途**            | 画像分類、物体検出、セグメンテーションなど幅広いコンピュータビジョンタスク。 | 画像分類、物体検出、セグメンテーション。特に大規模データセットでの高精度が強み。 |
| **スケーラビリティ** | 大規模データセット（ImageNet-21Kなど）でも良好なスケーリング。              | トランスフォーマーの特性により、超大規模データセットで特に優れる。            |

**解説**：
- **ConvNeXt**は、純粋なCNNでありながらViTと同等の精度を達成し、計算効率が高い。TPUやGPUでの推論が速く、実際のアプリケーションでの展開が容易。
- **CoAtNet**は、大規模データセット（例：JFT-300M）での事前学習により、特に高い汎化性能を発揮。トランスフォーマーの特性により、複雑なタスクや大規模データに強い。

D先生： “この調査結果によると、少なくとも設計思想から見てもConvNextとCoatNetは、同レベルでしょうね。プログラムに行きましょう。“

```python
######################################
# COATNETを使う
######################################
# 利用可能なCoAtNetモデルを確認
import timm
coatnet_models = [m for m in timm.list_models() if 'coatnet' in m and '224' in m]
print("利用可能なCoAtNetモデル:", coatnet_models[:5])  # 最初の5つを表示
# 利用可能なCoAtNetモデル: ['coatnet_0_224', 'coatnet_0_rw_224', 'coatnet_1_224', 'coat-net_1_rw_224', 'coatnet_2_224']

# ---
arch = "coatnet_0_rw_224"
print(f"\nFinding learning rate for {arch}")
find_lr(arch, item=Resize(192, method='squish'), 
        batch=aug_transforms(size=128, min_scale=0.75), bs=128)

learn_coatnet = train(arch, item=Resize(192, method='squish'),
                      batch=aug_transforms(size=128, min_scale=0.75), 
                      bs=128)

```

QEU:FOUNDER  ： “このモデルは、**ConvNextとほぼ同じ**です。ただし、CoatNetって計算に時間がかかりますね。まあ、少なく見ても、エッジコンピューティングに適するモデルではないです。”

![imageVATT2-5-11](/2025-09-03-QEUR23_VATT5/imageVATT2-5-11.jpg) 

D先生： “次は、**特徴量の可視化**をやってみましょう。ConvNetだけをやってみましょう。CoatNetのアテンションマップは、すでに作れますから・・・。まあ、CoatNetの場合は大したパフォーマンスがないので無視してもいいんじゃないですか？“

QEU:FOUNDER ： “おっと！すでにVibe Codingで調べたのですね。”

![imageVATT2-5-12](/2025-09-03-QEUR23_VATT5/imageVATT2-5-12.jpg) 

D先生： “予告ですが、可視化の結果も、なかなかの結果ですよ。 まあ、**ロボット化が視野に入っているご時世**なので、そんなことは当たり前なんですがね。“


## ～ まとめ ～

### ・・・ さあ、あのお方は？ ・・・

QEU:FOUNDER ： “あのお方は大したもんだなあ・・・。”

![imageVATT2-5-13](/2025-09-03-QEUR23_VATT5/imageVATT2-5-13.jpg) 

C部長 : “この人（↑）ですか？最近、バタバタしていますね。”

QEU:FOUNDER ： “違うちがう・・・。”

![imageVATT2-5-14](/2025-09-03-QEUR23_VATT5/imageVATT2-5-14.jpg) 

D先生：“この人ですか？極端な発現が多かったですが、**ある種の「サプリ」の作用**であったことがわかりました。”

[![MOVIE1](http://img.youtube.com/vi/6gAb8jiV630/0.jpg)](http://www.youtube.com/watch?v=6gAb8jiV630 "自民党や立憲民主党だけでなく、れいわも共産党も「田舎のおっさん」に足を引っ張られすぎなのである:15分朝刊チェック")

QEU:FOUNDER ： “どちみち正味切れ間近だったんですけとね。いやいや・・・。この人（↓）ですよ。”

![imageVATT2-5-15](/2025-09-03-QEUR23_VATT5/imageVATT2-5-15.jpg) 

D先生：“すべては、**彼から始まりましたね**。”
