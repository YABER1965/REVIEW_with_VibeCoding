---
title: QEUR23_VATT0 – 楽ちんに画像処理をやってのける（アテンション・マップ）
date: 2025-08-28
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "Vibe Coding", "Transformers"]
excerpt: いままでの成果をVibe Codingでレビューする
---

## QEUR23_VATT0 – 楽ちんに画像処理をやってのける（アテンション・マップ）

## ～ 今まで、何だったんだ・・・ ～

D先生（設定年齢65歳） ： “もともと、RAGとLLMを使いこなすBONSAI4をやるところが、閑話休題（占いをやってみた）に寄り道し、最後にBONSAIシステムの開発自体をやめてしまいました。”

![imageVATT2-1-1](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-1.jpg) 

QEU:FOUNDER（設定年齢65歳） ： “Vibe Codingが進んだ現在では、あまり面白みのないプロジェクトです。ただし、いつかはやるかもしれません。今から、「レビュー」をやりますから・・・。今の我々には、「汎用技術」があります。昔は、これがなくて四苦八苦していました。同じことを、汎用技術を前提にやってみたら、一体どのようなものができるのでしょうか・・・。”

D先生： “なるほど・・・。我々は、あの頃、**苦し紛れに間違ったやり方をしていた**ということも多々ありえますよね。具体的には、どのような感じになりそうですか？”


- 外観検査自動機
- 強化学習（たぶん、2048とマインスイーパーあたり）
- LLM(BONSAIが入るかもしれん)


QEU:FOUNDER ： “共通した考えは、**「モデル見直し」**です。今だったら、もっといいモデルを使うことができ、より良い結果が生まれるのではないでしょうか。”

C部長： “トランスフォーマーのこと？外観検査では使えるが、強化学習でトランスフォーマーが使えるのでしょうか？”

![imageVATT2-1-2](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-2.jpg) 

D先生 ： “今では、若干ですがTransformerも強化学習に使われるようです。なんでTransformerが因果関係モデルに使えるのか・・・。あかん・・・、ぜんぜんイメージがわかない。”

[![MOVIE1](http://img.youtube.com/vi/OeGkGdFmmr4/0.jpg)](http://www.youtube.com/watch?v=OeGkGdFmmr4 "テスラ オプティマス Gen 2 新バージョン イーロンのAIロボット、人間の仕事を代替")

QEU:FOUNDER ： “CNN（畳み込みニューラルネット）程度で、これほどの複雑なシステムが作れると思えないです。そもそも、CNNでは、マルチモーダルが出来ないでしょう。さて、今回の初回は、アテンション・マップをやってみましょう。それでは、たたき台のプログラムをドン・・・。”

```python
# ---
import torch
import numpy as np
import matplotlib.pyplot as plt
import cv2
from PIL import Image
from transformers import ViTImageProcessor, ViTForImageClassification

# GPUの可用性確認
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if device.type == "cuda":
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# 1. モデルとプロセッサのロード
model_name = "google/vit-base-patch16-224"
processor = ViTImageProcessor.from_pretrained(model_name)
model = ViTForImageClassification.from_pretrained(
    model_name,
    output_attentions=True  # アテンション重みを出力する設定
).to(device)
model.eval()  # 推論モードに設定

# ---
# 2. 画像のロードと前処理
image_path = "IMG_7469_2.JPG"  # 画像パスを指定
try:
    image = Image.open(image_path)
    print(f"Original image size: {image.size} ({image.format})")
except Exception as e:
    raise ValueError(f"画像の読み込みに失敗: {e}")

# モデルの入力サイズにリサイズ (224x224)
inputs = processor(images=image, return_tensors="pt")
inputs = {k: v.to(device) for k, v in inputs.items()}  # GPUに転送

# 3. 推論実行
with torch.no_grad():
    outputs = model(**inputs)

# 4. 予測結果の取得
logits = outputs.logits
predicted_class_idx = logits.argmax(-1).item()
predicted_label = model.config.id2label[predicted_class_idx]
print(f"予測結果: {predicted_class_idx} - {predicted_label}")

# 5. アテンションマップの生成
# 最終層のアテンション重みを取得 [batch, heads, tokens, tokens]
attentions = outputs.attentions
last_layer_attentions = attentions[-1].squeeze(0)  # [heads, tokens, tokens]

# CLSトークン (index=0) のアテンションを抽出
# tokens = 197 (196 patches + 1 CLS)
cls_attentions = last_layer_attentions[:, 0, 1:]  # [heads, patches]
cls_attentions = torch.mean(cls_attentions, dim=0)  # ヘッド方向に平均化

# 14x14のグリッドに再形成 (224/16=14)
grid_size = int(cls_attentions.numel() ** 0.5)
heatmap = cls_attentions.reshape(grid_size, grid_size).cpu().numpy()

# 224x224にアップサンプリング
upsampled_heatmap = cv2.resize(
    heatmap, 
    (224, 224), 
    interpolation=cv2.INTER_CUBIC
)
upsampled_heatmap = (upsampled_heatmap - upsampled_heatmap.min()) / (upsam-pled_heatmap.max() - upsampled_heatmap.min())

# 6. 入力画像の取得と正規化解除
input_image = inputs['pixel_values'].squeeze(0).cpu()
input_image = input_image.permute(1, 2, 0).numpy()  # CHW → HWC
# ViTの正規化を解除 ([0.5, 0.5, 0.5]で正規化されているため)
input_image = (input_image * np.array([0.5, 0.5, 0.5])) + np.array([0.5, 0.5, 0.5])
input_image = np.clip(input_image, 0, 1)

# 7. 可視化
plt.figure(figsize=(12, 6))

# 元画像とアテンションマップの重ね合わせ
plt.subplot(1, 2, 1)
plt.imshow(input_image)
plt.imshow(upsampled_heatmap, alpha=0.5, cmap='jet')
plt.title(f"Attention Map\n{predicted_label}", fontsize=14)
plt.axis('off')

# 単独のアテンションマップ
plt.subplot(1, 2, 2)
plt.imshow(upsampled_heatmap, cmap='jet')
plt.title("Raw Attention Heatmap", fontsize=14)
plt.colorbar(fraction=0.046, pad=0.04)
plt.axis('off')

plt.tight_layout()
plt.savefig("attention_result.png", dpi=300, bbox_inches='tight')
print("結果を保存: attention_result.png")
plt.show()

```

D先生 ： “なんか、変な結果になっていますよね。ちなみに、アテンションマップはうまくできていると思いますが・・・。”

![imageVATT2-1-3](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-3.jpg) 

C部長： “なつかしい・・・。ちなみに、これは昔の「コピペ」ですか？”

D先生 ： “Vibe Codingを使いました。**アッと言う間にプログラムが出来ます**。あの頃の苦しみが何だったのだろうと・・・（笑）。”

![imageVATT2-1-4](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-4.jpg) 

QEU:FOUNDER ： “それにしても、変な結果なんですよ。画像のど真ん中にネコちゃんがいるのに、認識してくれないんです。代わりに、トイレとか、洗濯機だって・・・（笑）。”

D先生 ： “このモデルに欠陥があるんでしょうね。ファインチューニングが必要です。”

QEU:FOUNDER ： “じゃあ、D先生・・・。次のファインチューニングも、Vibe Codingを使って、さくさくと終わらせておいてください。”


## ～ まとめ ～

### ・・・ 時代の変わり目です。本当に目まぐるしい ・・・

QEU:FOUNDER ： “この人がTwitterで怒っているのを見て、びっくりしました。”

![imageVATT2-1-5](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-5.jpg) 

D先生： “この人は、知る人ぞ知る有名人じゃないですか！！この人を怒らせるとは・・・。”

![imageVATT2-1-6](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-6.jpg) 

D先生： “彼は世界中の産業を見ているから、こんな感じの印象を受けるのはしようがないです。この国では、もうこの先、新しいものは生まれない。通常は、新しい製品ができて、初めてプロセスが変わり、そこで使うツールも変わるものです。”

QEU:FOUNDER ： “逆に言うと、**売り物がプロセスだけの商売（委託）なんかは、すでに大変なことになっている**ようです。。”

![imageVATT2-1-7](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-7.jpg) 

D先生： “うへえ、これはキツイ・・・。”

![imageVATT2-1-8](/2025-06-01-QEUR23_EOHS2/imageVATT2-1-8.jpg) 

QEU:FOUNDER ： “人手不足をテクノロジの進化で補うのが、王道ですね。”

