---
title: QEUR23_VATT2 - COATNETを使ったファインチューニング
date: 2025-08-31
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "Vibe Coding", "Transformers"]
excerpt: いままでの成果をVibe Codingでレビューする
---

## QEUR23 _VATT3 - COATNETを使ったアテンションマップ

## ～ Vibe Codingのすごさを見た！！ ～

### ・・・ 前回のつづきです ・・・

D先生（設定年齢65歳）： “そういえば、トライアルの途中で、こんなメッセージ（↓）が出てきました。”

### ✅ 4. アテンションマップの可視化（`attention_map_coatnet.py`）

> CoAtNetは通常、Attentionマップを直接出力しないが、`timm`で構築された場合、**中間層の特徴マップ**や**Grad-CAM**で代替的に可視化可能です。

### Grad-CAM を使用した可視化例：

QEU:FOUNDER（設定年齢65歳）  ： “あれ？COATNETは、アテンションマップを作画できないの？”

![imageVATT2-4-1](/2025-09-01-QEUR23_VATT4/imageVATT2-4-1.jpg) 

D先生： “とりあえず、**Grad-CAM** でやってみるしかないですね。そこで、なんとかCAMという可視化をやってみました。さて、プログラムをドン！！”

```python
# ---
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image

# モデル定義（ファインチューニング時と同一）
class CoAtNetForPetClassification(nn.Module):
    def __init__(self, num_classes=12, model_name='coatnet_0_rw_224'):
        super().__init__()
        from timm import create_model
        self.backbone = create_model(model_name, pretrained=False, num_classes=0)
        self.classifier = nn.Linear(self.backbone.num_features, num_classes)

    def forward(self, x):
        x = self.backbone(x)
        x = self.classifier(x)
        return x

# ---
class_names = ["Abyssinian",
        "American Bulldog",
        "American Pit Bull Terrier",
        "Basset Hound",
        "Beagle",
        "Bengal",
        "Birman",
        "Bombay",
        "Boxer",
        "British Shorthair",
        "Chihuahua",
        "Egyptian Mau",]

# ---
# モデルのロード
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CoAtNetForPetClassification(num_classes=12, model_name='coatnet_0_rw_224')
model.load_state_dict(torch.load('coatnet_cats_finetuned.pth', map_location=device))
model.to(device)
model.eval()

# ---
# 画像前処理
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ---
# 画像ロード
image_path = 'IMG_beagle.jpg'  # ← 任意の画像パス
image = Image.open(image_path).convert("RGB")
input_tensor = transform(image).unsqueeze(0).to(device)

# ---
# 推論
with torch.no_grad():
    outputs = model(input_tensor)
    probs = torch.softmax(outputs, dim=1)
    top_probs, top_indices = torch.topk(probs, 5)  # 上位5クラス

print("=== 予測上位5クラス ===")
for i in range(5):
    idx = top_indices[0][i].item()
    prob = top_probs[0][i].item()
    print(f"{i+1}. {class_names[idx]} ({prob:.1%})")

# ---
# Grad-CAMの設定（use_cuda引数を削除）
from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget

target_layers = [model.backbone.stages[-1]]  # 最後の特徴マップ層
cam = GradCAM(model=model, target_layers=target_layers)  # use_cudaは削除

# ---
# 元画像をnumpyに変換（0-1正規化）
rgb_img = np.array(image.resize((224, 224)), dtype=np.float32) / 255.0

# ---
# 上位5クラスのGrad-CAMを生成
cams = []
labels = []
for i in range(5):
    target_class = top_indices[0][i].item()
    target_label = class_names[target_class]
    labels.append(target_label)
    
    # targetsをClassifierOutputTargetでラップ
    targets = [ClassifierOutputTarget(target_class)]
    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)
    grayscale_cam = grayscale_cam[0, :]
    cams.append(grayscale_cam)

# ---
# 可視化（3行×4列のグリッド）
fig, axes = plt.subplots(2, 4, figsize=(16, 12))
axes = axes.flatten()

# 元画像（4回表示）
for i in range(4):
    axes[i].imshow(rgb_img)
    axes[i].set_title("Original Image")
    axes[i].axis("off")

# Grad-CAM（上位4クラス）
for i in range(4):
    cam_image = show_cam_on_image(rgb_img, cams[i], use_rgb=True)
    axes[4 + i].imshow(cam_image)
    axes[4 + i].set_title(f"Grad-CAM: {labels[i]}")
    axes[4 + i].axis("off")

# 5番目のクラスは別ウィンドウで表示（任意）
plt.tight_layout()
plt.savefig("multiclass_gradcam_output.png")
plt.show()

```

QEU:FOUNDER  ： “Grad-CAMって、どんなものなのでしょう。わくわく・・・。”

![imageVATT2-4-2](/2025-09-01-QEUR23_VATT4/imageVATT2-4-2.jpg) 

D先生： “確かに、見た目はアテンション・マップに似ています。この場合、判別の根拠を可視化したわけです。相対的には、ピーグル犬の全体に薄っすらとした青色が覆っていることから、CNNモデルが適切に認識していることがわかります。”

QEU:FOUNDER  ： “でもねえ・・・。赤い色の領域が、判別対象の外にあるでしょう？ちょっと不満です。もう少し改善できませんか？”

![imageVATT2-4-3](/2025-09-01-QEUR23_VATT4/imageVATT2-4-3.jpg) 

QEU:FOUNDER  ： “さきほどのGrad-CAMのプログラムがCOATNETのモデル構造を吐き出してくれました。これをAIに教えてみればアテンション・マップを作ってくれるんじゃないですか？さきほど、AIは、「モデルの構造がわからないのでアテンション・マップが作れません」と言っていましたよ。”

**（さらにAIで調査中・・・）**

D先生： “なるほど、そういう考え方もありますよね。このモデルの構造を見ていると、本当に後半がトランスフォーマーになっているんですね。”

**(モデル構造の中身)**

```python
"""
CoAtNetForPetClassification(
  (backbone): MaxxVit(
    (stem): Stem(
      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (norm1): BatchNormAct2d(
        32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True
        (drop): Identity()
        (act): SiLU(inplace=True)
      )
      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    )

# ・・・ 途中は省略 ・・・

      (3): MaxxVitStage(
        (blocks): Sequential(
          (0): TransformerBlock2d(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
              (expand): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
            )
            (norm1): Sequential(
              (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
              (down): Downsample2d(
                (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
                (expand): Identity()
              )
            )
            (attn): Attention2d(
              (qkv): Conv2d(384, 1152, kernel_size=(1, 1), stride=(1, 1))
              (rel_pos): RelPosBias()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1))
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
            (mlp): ConvMlp(
              (fc1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
              (norm): Identity()
              (act): GELU()
              (drop): Dropout(p=0.0, inplace=False)
              (fc2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
          (1): TransformerBlock2d(
            (shortcut): Identity()
            (norm1): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
            (attn): Attention2d(
              (qkv): Conv2d(768, 2304, kernel_size=(1, 1), stride=(1, 1))
              (rel_pos): RelPosBias()
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (drop_path1): Identity()
            (norm2): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
            (mlp): ConvMlp(
              (fc1): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
              (norm): Identity()
              (act): GELU()
              (drop): Dropout(p=0.0, inplace=False)
              (fc2): Conv2d(3072, 768, kernel_size=(1, 1), stride=(1, 1))
            )
            (ls2): Identity()
            (drop_path2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)
    (head): ClassifierHead(
      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))
      (drop): Dropout(p=0.0, inplace=False)
      (fc): Identity()
      (flatten): Identity()
    )
  )
  (classifier): Linear(in_features=768, out_features=12, bias=True)
)
"""
```

QEU:FOUNDER  ： “アテンション・マップができましたか？”


**はい、その情報は**CoAtNetの内部構造**を示しており、**アテンションマップを描くことが可能です**！**

### 🔍 重要なポイント

1. **TransformerBlock2d** が存在する
2. 各ブロックに **Attention2d** レイヤーがある
3. **qkv**（Query, Key, Value）の畳み込みが定義されている

つまり、**ViTのような明示的なSelf-Attention**を持ち、**アテンション重みを取得可能**です！

D先生： “できました！・・・ていうか、AIが「できました」といってます！！それでは、プログラムをドン！！”

```python
# ---
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import cv2

# モデル定義（ファインチューニング時と同一）
class CoAtNetForPetClassification(nn.Module):
    def __init__(self, num_classes=12, model_name='coatnet_0_rw_224'):
        super().__init__()
        from timm import create_model
        self.backbone = create_model(model_name, pretrained=False, num_classes=0)
        self.classifier = nn.Linear(self.backbone.num_features, num_classes)

    def forward(self, x):
        x = self.backbone(x)
        x = self.classifier(x)
        return x

# ---
class_names = ["Abyssinian",
        "American Bulldog",
        "American Pit Bull Terrier",
        "Basset Hound",
        "Beagle",
        "Bengal",
        "Birman",
        "Bombay",
        "Boxer",
        "British Shorthair",
        "Chihuahua",
        "Egyptian Mau",]

# ---
# モデルのロード
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CoAtNetForPetClassification(num_classes=12, model_name='coatnet_0_rw_224')
model.load_state_dict(torch.load('coatnet_cats_finetuned.pth', map_location=device))
model.to(device)
model.eval()

# ---
# 画像前処理
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# ---
# 画像ロード
image_path = 'IMG_beagle.jpg'  # ← 任意の画像パス
image = Image.open(image_path).convert("RGB")
input_tensor = transform(image).unsqueeze(0).to(device)

# ---
# 推論
with torch.no_grad():
    outputs = model(input_tensor)
    probs = torch.softmax(outputs, dim=1)
    predicted_idx = probs.argmax(1).item()
    predicted_label = class_names[predicted_idx]
    confidence = probs[0][predicted_idx].item()

print(f"予測結果: {predicted_label} (確信度: {confidence:.2%})")

# ---
# アテンションマップを取得するためのフック
attention_maps = []

def hook_fn(module, input, output):
    try:
        # Attention2dの入力は [B, C, H, W] の特徴マップ
        x = input[0]  # [B, C, H, W]
        B, C, H, W = x.shape
        N = H * W  # パッチ数
        
        # qkvはConv2d(384, 1152, kernel_size=(1, 1))のような構造
        # つまり、入力C -> 出力3*C (Q, K, V)
        
        # qkvの重みを取得してアテンションを計算
        if hasattr(module, 'weight'):
            qkv_weight = module.weight  # [3*C_out, C_in, 1, 1]
            qkv_bias = module.bias if module.bias is not None else None
            
            # QKVを計算
            qkv = torch.nn.functional.conv2d(x, qkv_weight, qkv_bias, stride=1, padding=0)
            # qkv: [B, 3*C_out, H, W]
            
            _, C_total, _, _ = qkv.shape
            C_per_qkv = C_total // 3
            
            # Q, K, Vに分割
            q, k, v = qkv.split(C_per_qkv, dim=1)
            
            # パッチ次元に変形: [B, C, H, W] -> [B, C, N] -> [B, N, C]
            q = q.view(B, C_per_qkv, N).permute(0, 2, 1)  # [B, N, C]
            k = k.view(B, C_per_qkv, N).permute(0, 2, 1)  # [B, N, C]
            
            # アテンションスコア計算: [B, N, N]
            attn_scores = torch.matmul(q, k.transpose(-2, -1))  # [B, N, N]
            attn_scores = attn_scores / (C_per_qkv ** 0.5)  # スケーリング
            attn_weights = torch.softmax(attn_scores, dim=-1)   # [B, N, N]
            
            # 各パッチがどこに注目しているかの平均を取る
            mean_attention = attn_weights.mean(dim=1)  # [B, N]
            
            attention_maps.append(mean_attention.squeeze().cpu().numpy())
    except Exception as e:
        print(f"アテンション取得エラー: {e}")
        pass

# ---
# アテンションレイヤーにフックを登録
hooks = []
target_modules = []

# Attention2dモジュールを探索
for name, module in model.named_modules():
    if 'attn' in name and hasattr(module, 'weight') and len(module.weight.shape) == 4:
        # Conv2dで、かつAttentionのqkv畳み込み
        if 'qkv' in name or (module.weight.shape[1] > 64):  # 適当な条件でフィルタ
            target_modules.append((name, module))

# 最後のAttentionレイヤーのみにフックを登録
if target_modules:
    last_module = target_modules[-1][1]  # 最後のモジュール
    hooks.append(last_module.register_forward_hook(hook_fn))
    print(f"フック登録: {target_modules[-1][0]}")
else:
    print("適切なAttentionモジュールが見つかりませんでした")

# ---
# 再推論（フックでアテンションを取得）
attention_maps.clear()
with torch.no_grad():
    outputs = model(input_tensor)

# フック解除
for hook in hooks:
    hook.remove()

# ---
# アテンションマップを可視化
if attention_maps and len(attention_maps) > 0:
    # 最後のアテンションマップを使用
    attn_map = attention_maps[-1]  # [N]
    
    # グリッドサイズを計算 (例: 14x14, 7x7 など)
    N = len(attn_map)
    grid_size = int(np.sqrt(N))
    
    # リシェイプ (もし平方数でない場合は調整)
    if grid_size * grid_size == N:
        attn_map = attn_map.reshape(grid_size, grid_size)
    else:
        # 最も近い正方形サイズに調整
        grid_size = int(np.ceil(np.sqrt(N)))
        # padding
        padded_attn = np.pad(attn_map, (0, grid_size * grid_size - N), mode='constant')
        attn_map = padded_attn.reshape(grid_size, grid_size)
    
    # 正規化
    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())
    
    # アップサンプリング
    attn_upscaled = cv2.resize(attn_map, (224, 224), interpolation=cv2.INTER_LINEAR)
    
    # 元画像をnumpyに変換
    img_np = np.array(image.resize((224, 224)), dtype=np.float32) / 255.0
    
    # ---
    # 可視化
    plt.figure(figsize=(15, 5))
    
    # 元画像
    plt.subplot(1, 3, 1)
    plt.imshow(img_np)
    plt.title("Original Image")
    plt.axis("off")
    
    # アテンションマップ
    plt.subplot(1, 3, 2)
    plt.imshow(attn_upscaled, cmap='jet')
    plt.title("Attention Map")
    plt.axis("off")
    
    # 重ね合わせ
    plt.subplot(1, 3, 3)
    plt.imshow(img_np)
    plt.imshow(attn_upscaled, cmap='jet', alpha=0.5)
    plt.title(f"Attention Overlay\n{predicted_label} ({confidence:.1%})")
    plt.axis("off")
    
    plt.tight_layout()
    plt.savefig("coatnet_attention_map.png")
    plt.show()
    
    print("CoAtNetアテンションマップを保存しました: coatnet_attention_map.png")
else:
    print("アテンションマップを取得できませんでした。")
    # 代替として特徴量マップを表示
    print("代替として特徴量マップを生成します...")
    
    with torch.no_grad():
        features = model.backbone.forward_features(input_tensor)
        if len(features.shape) == 4:
            feature_map = features.mean(dim=1).squeeze().cpu().numpy()  # [H, W]
        else:
            # トークン形式
            B, N, C = features.shape
            H = W = int(np.sqrt(N))
            feature_map = features.mean(dim=-1).view(B, H, W).squeeze().cpu().numpy()
        
        # 正規化とリサイズ
        feature_map = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min())
        feature_map_upscaled = cv2.resize(feature_map, (224, 224), interpola-tion=cv2.INTER_LINEAR)
        
        # 可視化
        plt.figure(figsize=(15, 5))
        
        img_np = np.array(image.resize((224, 224)), dtype=np.float32) / 255.0
        
        plt.subplot(1, 3, 1)
        plt.imshow(img_np)
        plt.title("Original Image")
        plt.axis("off")
        
        plt.subplot(1, 3, 2)
        plt.imshow(feature_map_upscaled, cmap='jet')
        plt.title("Feature Map")
        plt.axis("off")
        
        plt.subplot(1, 3, 3)
        plt.imshow(img_np)
        plt.imshow(feature_map_upscaled, cmap='jet', alpha=0.5)
        plt.title(f"Feature Overlay\n{predicted_label} ({confidence:.1%})")
        plt.axis("off")
        
        plt.tight_layout()
        plt.savefig("coatnet_feature_map.png")
        plt.show()
        
        print("特徴量マップを保存しました: coatnet_feature_map.png")

```

D先生： “確かに、それらしいものが出来ています。”

![imageVATT2-4-4](/2025-09-01-QEUR23_VATT4/imageVATT2-4-4.jpg) 

QEU:FOUNDER ： “ああ・・・。Grad-CAMよりも納得性が高いです。・・・でもねえ、ビーグル犬の判別をしたいのであれば、普通は、あの特色のある耳の部分に注目するでしょうに・・・。”

D先生： “たしかにね・・・。ただし、このプログラムもまだまだ改善の余地があるでしょう。”

### モデルの概要と事前学習について

### 1. **モデルの事前学習状況**
- **モデル**: CoAtNet（Convolutional and Transformer Network）
  - このプログラムでは、`timm`ライブラリを使用してCoAtNetモデル（具体的には`coatnet_0_rw_224`など）をロードしています。
  - `pretrained=True`が指定されており、事前学習済みのモデルがデフォルトで使用されます。事前学習済みモデルが見つからない場合は、ランダム初期化（`pretrained=False`）にフォールバックします。
- **事前学習データセット**:
  - `timm`ライブラリで提供されるCoAtNetモデルは、通常 **ImageNet-1k**（約100万枚の画像、1000クラス）で事前学習されています。一部のモデル（例: `coatnet_0_rw_224`）は、ImageNet-1kに加えて追加のデータ（例: ImageNet-21k や独自のデータセット）で事前学習されている場合もありますが、詳細は`timm`のドキュメントやモデル固有のメタデータに依存します。
  - 事前学習済みの重みは、画像分類タスクで一般的な特徴量抽出能力をモデルに付与します。
- **ファインチューニング**:
  - このプログラムでは、事前学習済みのCoAtNetモデルを**Oxford-IIIT Petデータセット**（猫の品種12クラスに限定）でファインチューニングしています。ファインチューニングにより、モデルは猫の品種分類に特化した特徴を学習します。

### 2. **モデルの構造**
CoAtNetは、**Convolutional Neural Network（CNN）**と**Vision Transformer（ViT）**のハイブリッドアーキテクチャです。以下はモデルの構造の概要です：

- **バックボーン (self.backbone)**:
  - CoAtNetの主要部分で、`timm`ライブラリからロードされます（例: `coatnet_0_rw_224`）。
  - CoAtNetは、CNNの局所特徴抽出能力とTransformerのグローバルなコンテキスト捕捉能力を組み合わせた構造を持ちます。具体的には：
    - **初期層**: 畳み込み層（Conv Layers）で局所的な画像特徴（エッジやテクスチャなど）を抽出。
    - **中間層**: Transformerブロックを導入し、自己注意機構（Self-Attention）を使用して画像全体の長距離依存関係をモデル化。
    - **後半層**: さらに畳み込みとTransformerを組み合わせて、効率的かつ高精度な特徴表現を構築。
  - `num_classes=0`が指定されているため、バックボーンの最終分類層（通常の全結合層）は除去され、特徴量ベクトル（`self.backbone.num_features`次元）のみが出力されます。
  - 例: `coatnet_0_rw_224`の場合、出力特徴量次元は通常768次元程度（モデルによって異なる）。

- **分類ヘッド (self.classifier)**:
  - `nn.Linear(self.backbone.num_features, num_classes)`で定義される全結合層。
  - バックボーンから出力された特徴量ベクトルを、12クラス（猫の品種）に対応する出力に変換します。
  - この層は、ファインチューニングの際に新たに追加された部分であり、ランダムに初期化されます。

- **全体の流れ**:
  - 入力画像（224x224ピクセル、3チャネル）が`self.backbone`に入力され、特徴量ベクトルに変換。
  - この特徴量ベクトルが`self.classifier`に渡され、12クラスのロジット（分類スコア）を出力。
  - 出力は`CrossEntropyLoss`で損失計算に使用される。

### 3. **学習しているパラメータ**
このプログラムでは、**モデル全体（バックボーン + 分類ヘッド）**のパラメータが学習対象です。具体的には：
- **バックボーン (`self.backbone`)**:
  - 事前学習済みの重みで初期化されていますが、ファインチューニング中はすべての層（畳み込み層、Transformerブロックなど）が学習可能（`requires_grad=True`）です。
  - `optimizer = AdamW(model.parameters(), lr=2e-5)`により、バックボーンの全パラメータが更新されます。
  - ただし、学習率が低く設定されている（`lr=2e-5`）ため、事前学習済みの重みは大きく変化せず、微調整される形になります。
- **分類ヘッド (`self.classifier`)**:
  - 新たに追加された全結合層であり、ランダムに初期化されています。
  - この層のパラメータは、ファインチューニング中に最も大きく更新されます。猫の品種分類に特化した特徴を学習するためです。
- **学習パラメータの割合**:
  - CoAtNetモデルのパラメータ数はモデルサイズに依存します（例: `coatnet_0_rw_224`で約25Mパラメータ）。
  - 分類ヘッドのパラメータ数は、`self.backbone.num_features * num_classes`（例: 768 * 12 = 9,216）にバイアス項を加えた程度であり、全体のパラメータ数に比べると非常に少ない。
  - したがって、学習の大部分はバックボーンの微調整に依存しますが、分類ヘッドの学習がタスク特化の性能向上に大きく寄与します。

### 4. **データセットについて**
- **データセット**: Oxford-IIIT Petデータセット
  - **内容**: 37種類のペット（猫と犬）の画像データセット。プログラムでは猫の品種12クラスに限定。
  - **分割**:
    - トレーニング/バリデーション用（`trainval`）：約3,680画像。
    - テスト用（`test`）：約3,669画像。
    - 猫の品種12クラスに限定することで、データ量はさらに削減（約1,440画像程度、クラスあたり約120画像）。
  - **前処理**:
    - 画像は224x224ピクセルにリサイズされ、テンソルに変換（`transforms.Resize`, `trans-forms.ToTensor`）。
    - データ拡張（例: ランダムフリップや回転）はこのコードでは適用されていません。
- **カスタムデータセット**:
  - `CatOnlyOxfordIIITPet`クラスを使用して、猫の品種のみを抽出。
  - クラスインデックスを再マッピングし、12クラスに対応。

QEU:FOUNDER ： “モデル全体がチューニングの対象になっていたんですね。ともあれ、現時点で発見された一番の問題点は、このシステムは異常検出に対応できないことです。”

![imageVATT2-4-5](/2025-09-01-QEUR23_VATT4/imageVATT2-4-5.jpg) 

D先生： “新しいシステムは、農業問題（↑）のようなテーマに対応できること。せっかく画像サイズの制約の緩やかなCNNを使っているのだから、より大きな画像にも対応できることが必要ですね。”

QEU:FOUNDER ： “このプロジェクトは、まだまだ助走の段階です。それにしても、Vibe Codingを使うと展開が速いねえ。”


## ～ まとめ ～

C部長 : “今回は、AIが一旦はできないと判断したアテンションマップの生成を可能にしました。これは、非常に画期的なことです。”

![imageVATT2-4-6](/2025-09-01-QEUR23_VATT4/imageVATT2-4-6.jpg) 

QEU:FOUNDER ： “Vibe Codingというのは、たかがメイズ生成の道具だと思っていました。”

![imageVATT2-4-7](/2025-09-01-QEUR23_VATT4/imageVATT2-4-7.jpg) 

QEU:FOUNDER ： “それでも、イノベーションが起きるケースがまれにあるらしい。重要なのは**AIもユーザーも知らない知見がインプットされること**ですね。”

C部長 : “他には、どんなケースがあるんでしょうか？”

[![MOVIE1](http://img.youtube.com/vi/dQbFqMgm-lQ/0.jpg)](http://www.youtube.com/watch?v=dQbFqMgm-lQ "松田語録：エマージェンスと知能")

QEU:FOUNDER ： “まあ、少なくとも一つ言えるのは、「実験（実際にPCで動かしてみた）」とかは、新しい知見のトリガーになるんでしょうね。何はともあれ、今後の展開が面白くなってきました。”

