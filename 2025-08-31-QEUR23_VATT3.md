---
title: QEUR23_VATT2 - COATNETを使ったファインチューニング
date: 2025-08-31
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "Vibe Coding", "Transformers"]
excerpt: いままでの成果をVibe Codingでレビューする
---

## QEUR23_VATT2 - COATNETを使ったファインチューニング

## ～ 何しろ、判別モデルを軽くしたい ～

QEU:FOUNDER（設定年齢65歳） ： “前回は、Vibe Codingを使って、簡単にアテンションマップを作ることをやりました。今回から、いよいよ「本題」になります。”

![imageVATT2-3-1](/2025-08-31-QEUR23_VATT3/imageVATT2-3-1.jpg) 

D先生（設定年齢65歳）： “アテンションマップ以上の新しいことを？へ？何をやるんですか？”

**（特徴量の抽出）**

![imageVATT2-3-2](/2025-08-31-QEUR23_VATT3/imageVATT2-3-2.jpg) 

**（アテンションマップの生成）**

![imageVATT2-3-3](/2025-08-31-QEUR23_VATT3/imageVATT2-3-3.jpg) 

QEU:FOUNDER ： “我々がいままで取り組んできた外観検査自動機のプロジェクトのうち、最も成功したのはNSOARTCの特徴量とViTモデルの組み合わせを使った異常検出モデルでした。ただし、この考え方は、やりかたがあまりにも特殊過ぎて実用的ではないです。面白いけどね・・・（笑）。だから、ViTモデルの代わりに、CNNモデルとViTモデルを組み合わせたモデルを使いたい。”

D先生： “そういうモデルはあるんですか？”

| モデル名 | 特徴 | ImageNet精度 | 計算量 (GFLOPs) | 優位性 |
|----------|------|--------------|-----------------|--------|
| **CoAtNet** | CNNとトランスフォーマーを段階的に組み合わせ | 86.0% | 4.9 | ViTより1.5倍高速、精度向上 |
| **MobileViT** | 軽量CNN + 軽量トランスフォーマー | 78.4% | 1.2 | モバイルデバイス向け最適化 |
| **ConViT** | CNNの誘導的バイアスをトランスフォーマーに導入 | 81.3% | 4.3 | 小規模データで優れた汎化性能 |
| **CMT** | CNNの局所性とトランスフォーマーのグローバル性を統合 | 83.5% | 2.7 | 高精度かつ低計算負荷 |

**実績データ**:
- CoAtNet-0 (ResNet相当) はViT-B/16比で**2.5倍高速**かつ**1.2%精度向上**
- MobileViT-S はMobileNetV3比で**2.3%精度向上**しながら計算量は**+15%** に抑えられる
- 小規模データセット（Oxford-IIIT Pets等）では、CNNバックボーンによる**データ効率の向上**が顕著（学習データ30%削減で同等精度）

### 2. プリトレーニングモデルの存在
- **CoAtNet**: TensorFlow Hubに公式実装あり（[CoAtNet-0](https://tfhub.dev/sayakpaul/coatnet)）
- **MobileViT**: PyTorch公式実装あり（[mobile-vit-pytorch](https://github.com/apple/ml-cvnets)）
- **CMT**: Hugging Faceに転用可能モデル（[cvt-13](https://huggingface.co/nateraw/cvt-13)）
- **ただし**：Oxford-IIIT Pets向けのドメイン特化プリトレーニングモデルは存在せず、ImageNetプリトレーニングモデルのファインチューニングが標準アプローチ

QEU:FOUNDER  ： “AIに、**「ハイブリッドモデル」**について質問をしました。これを使ってみましょう。それでは、ファインチューニングのプログラムをVibe Codingをしてみました。今回は、プログラムをそのまま晒します。”

```python
# ---
# coatnet_model.py
import torch
import torch.nn as nn
from timm import create_model

class CoAtNetForPetClassification(nn.Module):
    def __init__(self, num_classes=12, model_name='coatnet_rmlp_0_rw_224'):
        super().__init__()
        # 修正: 利用可能なモデル名を使用
        try:
            self.backbone = create_model(model_name, pretrained=True, num_classes=0)
        except RuntimeError:
            print(f"事前学習済みモデル {model_name} が見つかりません。ランダム初期化します。")
            self.backbone = create_model(model_name, pretrained=False, num_classes=0)
        
        self.classifier = nn.Linear(self.backbone.num_features, num_classes)

    def forward(self, x):
        x = self.backbone(x)
        x = self.classifier(x)
        return x

```

QEU:FOUNDER ： “今回のVibe Codingシリーズでは、D先生にプログラムの解説をお願いします。それでは、どうぞ・・・。”

D先生： “えっ！？私がですか・・・。ここでは、モデルの定義をしています。本来は、このモデルは独立したpyファイルを形成していますが、今回はファインチューニング本体と一緒にしています。う～ん・・・。あと言うことは・・・。”

QEU:FOUNDER ： “このモデルは事前トレーニングされているんですか？”

D先生： “そうそう・・・。これは事前トレーニング済です。”

```python
# ---
# coatnet_finetune_with_evaluation.py
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets import OxfordIIITPet
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support
import seaborn as sns
from PIL import Image

# モデル定義
# from coatnet_model import CoAtNetForPetClassification

# 利用可能なCoAtNetモデルを確認
import timm
coatnet_models = [m for m in timm.list_models() if 'coatnet' in m and '224' in m]
print("利用可能なCoAtNetモデル:", coatnet_models[:5])  # 最初の5つを表示
# 利用可能なCoAtNetモデル: ['coatnet_0_224', 'coatnet_0_rw_224', 'coatnet_1_224', 'coat-net_1_rw_224', 'coatnet_2_224']

```

D先生： “CoAtNetモデルには、いくつかの種類があります。その種類を出力してみました。”

QEU:FOUNDER ： “今回、そのうちの一つを選択しました。その選択の根拠は？”

D先生： “このモデルの特長についてAIに質問しました。全然、訳が分かりませんでした。ただ、適当に１つを入力したら、エラーがでなかったですから、これでOKかなっと・・・。”

QEU:FOUNDER ： “**Vibe Coding(ノリでコーディング)**らしくて、とても結構です。”

```python
# ---
# データセット
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# クラス名の取得（猫の品種のみ）
full_dataset = OxfordIIITPet(root='./data', split='trainval', target_types='category', download=True)
cat_indices = list(range(12))  # 最初の12クラスが猫の品種
class_names = [full_dataset.classes[i] for i in cat_indices]

# カスタムデータセットラッパー（猫のみ）
class CatOnlyOxfordIIITPet:
    def __init__(self, base_dataset, transform=None, return_image=False):
        self.base_dataset = base_dataset
        self.transform = transform
        self.return_image = return_image
        self.cat_indices = list(range(12))
        self.cat_mask = [i for i, (_, label) in enumerate(self.base_dataset) if label in self.cat_indices]
        self.label_map = {old_label: new_label for new_label, old_label in enumerate(self.cat_indices)}
        self.class_names = [self.base_dataset.classes[i] for i in self.cat_indices]

    def __len__(self):
        return len(self.cat_mask)

    def __getitem__(self, idx):
        actual_idx = self.cat_mask[idx]
        image, label = self.base_dataset[actual_idx]
        label = self.label_map[label]
        
        if self.transform:
            image = self.transform(image)
            
        if self.return_image:
            # 元の画像も返す（可視化用）
            original_image = self.base_dataset[actual_idx][0]
            return image, label, original_image
        else:
            return image, label

# データセットの準備
train_full = OxfordIIITPet(root='./data', split='trainval', target_types='category', download=True)
val_full = OxfordIIITPet(root='./data', split='test', target_types='category', download=True)

train_dataset = CatOnlyOxfordIIITPet(train_full, transform=transform)
val_dataset = CatOnlyOxfordIIITPet(val_full, transform=transform)
val_dataset_with_image = CatOnlyOxfordIIITPet(val_full, transform=transform, re-turn_image=True)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # バッチサイズを小さく
val_loader = DataLoader(val_dataset, batch_size=8)

# モデルと最適化（修正版）
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

```

D先生： “ここでは、言いたいことがあります。このデータセットは、犬猫のデータセットには様々なサイズの画像を使っています。これを224x224にリサイズします。”

QEU:FOUNDER ： “そのサイズはViTの制約ですね。それにしても、画像分類目的ではOKですが、異常検出目的であればサイズ不足になりますね。”

D先生： “あと、もう一つ・・・。**データ拡張（画像をカット、回転など）をしていません。**”

QEU:FOUNDER ： “精度には、十分改善の余地があるわけですね。”

```python
# ---
# 修正: 利用可能なモデル名を使用
# 利用可能なCoAtNetモデル: ['coatnet_0_224', 'coatnet_0_rw_224', 'coatnet_1_224', 'coat-net_1_rw_224', 'coatnet_2_224']
model_name='coatnet_0_rw_224'
model = CoAtNetForPetClassification(num_classes=12, model_name=model_name).to(device)
optimizer = AdamW(model.parameters(), lr=2e-5)
criterion = CrossEntropyLoss()

# ---
# 学習履歴の保存用
train_losses = []
val_losses = []
val_accuracies = []

# 学習ループ
for epoch in range(6):
    # トレーニング
    model.train()
    total_train_loss = 0
    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/6 [Train]'):
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()
    
    avg_train_loss = total_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    
    # 検証
    model.eval()
    total_val_loss = 0
    correct = 0
    
    with torch.no_grad():
        for images, labels in val_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_val_loss += loss.item()
            
            preds = outputs.argmax(1)
            correct += (preds == labels).sum().item()
    
    avg_val_loss = total_val_loss / len(val_loader)
    val_accuracy = correct / len(val_dataset)
    
    val_losses.append(avg_val_loss)
    val_accuracies.append(val_accuracy)
    
    print(f'Epoch {epoch+1}/6:')
    print(f'  Train Loss: {avg_train_loss:.4f}')
    print(f'  Val Loss: {avg_val_loss:.4f}')
    print(f'  Val Accuracy: {val_accuracy:.4f}')

# 保存
torch.save(model.state_dict(), 'coatnet_cats_finetuned.pth')
print("モデルを保存しました: coatnet_cats_finetuned.pth")

# 1. 学習曲線のプロット
def plot_training_curves2():
    plt.figure(figsize=(15, 5))
    
    # 損失曲線
    plt.subplot(1, 3, 1)
    plt.plot(train_losses, label='Train Loss', marker='o')
    plt.plot(val_losses, label='Validation Loss', marker='s')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # 精度曲線
    plt.subplot(1, 3, 2)
    plt.plot(val_accuracies, label='Validation Accuracy', marker='^', color='green')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    # 損失と精度の比較
    plt.subplot(1, 3, 3)
    plt.plot(val_losses, label='Validation Loss', marker='s')
    plt.plot([1-acc for acc in val_accuracies], label='Validation Error', marker='^')
    plt.title('Loss vs Error')
    plt.xlabel('Epoch')
    plt.ylabel('Value')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('training_curves.png')
    plt.show()
    print("学習曲線を保存しました: training_curves.png")

# 学習曲線のプロット
plot_training_curves2()

```

QEU:FOUNDER ： “やっと実験の結果が出てきました。これは性能としては悪くないんじゃないですか？”

![imageVATT2-3-4](/2025-08-31-QEUR23_VATT3/imageVATT2-3-4.jpg) 

D先生： “あっ・・・。ありがとうございます。”

QEU:FOUNDER ： “小生に礼を言ってどうする（笑）？”

```python
# ---
# 2. 10件の画像を表示し、予測結果を出力
def visualize_predictions(num_samples=10):
    model.eval()
    indices = np.random.choice(len(val_dataset_with_image), num_samples, replace=False)
    fig, axes = plt.subplots(2, 5, figsize=(15, 8))
    axes = axes.ravel()
    
    predictions = []
    true_labels = []
    
    for i, idx in enumerate(indices):
        image_tensor, label, original_image = val_dataset_with_image[idx]
        image_tensor = image_tensor.unsqueeze(0).to(device)
        
        with torch.no_grad():
            outputs = model(image_tensor)
            pred = outputs.argmax(1).item()
        
        predictions.append(pred)
        true_labels.append(label)
        
        # 画像の表示
        axes[i].imshow(original_image)
        axes[i].set_title(f'True: {class_names[label]}\nPred: {class_names[pred]}')
        axes[i].axis('off')
        
        print(f'画像 {i+1}: 実際={class_names[label]}, 予測={class_names[pred]}')
    
    plt.tight_layout()
    plt.savefig('prediction_samples.png')
    plt.show()
    print("予測サンプルを保存しました: prediction_samples.png")
    
    # サンプルデータの混同行列
    cm = confusion_matrix(true_labels, predictions)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=[class_names[i] for i in range(len(class_names))][:len(np.unique(predictions))], 
                yticklabels=[class_names[i] for i in range(len(class_names))][:len(np.unique(true_labels))])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix (Sampled Test Data)')
    plt.savefig('confusion_matrix_sampled.png')
    plt.show()
    print("サンプル混同行列を保存しました: confusion_matrix_sampled.png")

# 予測結果の可視化
visualize_predictions()

```

D先生： “このモデルの判別精度について、より直感的に分析できるように工夫しました。”

![imageVATT2-3-5](/2025-08-31-QEUR23_VATT3/imageVATT2-3-5.jpg) 

QEU:FOUNDER ： “幸運なことに、構造的な問題は、ほとんどなさそうですね。もちろん、学習データ数の少なさはあるが・・・。そういえば、猫だけに限定したんじゃないの？”

D先生： “このデータセットには、12種類の猫が登録されています。12種類だけ切り出せば、なんとかなると思っていました・・・（笑）。”

QEU:FOUNDER ： “小生は、犬も好きなので、OKとしましょう。”

```python
# ---
# 3. すべてのval_datasetを使って混同行列を計算
def evaluate_all_test_data():
    model.eval()
    all_predictions = []
    all_true_labels = []
    
    print("すべてのテストデータを評価中...")
    with torch.no_grad():
        for i, (images, labels) in enumerate(tqdm(val_loader)):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(1)
            all_predictions.extend(preds.cpu().numpy())
            all_true_labels.extend(labels.cpu().numpy())
    
    # 完全な混同行列
    cm = confusion_matrix(all_true_labels, all_predictions)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, 
                yticklabels=class_names)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix (All Test Data)')
    plt.savefig('confusion_matrix_all.png')
    plt.show()
    print("全体混同行列を保存しました: confusion_matrix_all.png")
    
    # メトリクスの計算と表示
    acc = accuracy_score(all_true_labels, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(all_true_labels, all_predictions, aver-age='weighted')
    
    print(f"\n=== 全体テスト結果 ===")
    print(f"Accuracy: {acc:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")

# 全データの評価
evaluate_all_test_data()

```

QEU:FOUNDER ： “お約束の混同行列ですね。一部だけですが、このファインチューニングの弱点が見られます。どうします？”

![imageVATT2-3-6](/2025-08-31-QEUR23_VATT3/imageVATT2-3-6.jpg) 

D先生： “Vibe(ノリ)なので、画像を含めてAIに質問してみました。”

![imageVATT2-3-7](/2025-08-31-QEUR23_VATT3/imageVATT2-3-7.jpg) 

D先生： “結果がコレ（↓）です。”

混同行列とパフォーマンス指標（Accuracy: 0.9205, Precision: 0.9273, Recall: 0.9205, F1-Score: 0.9200）を見る限り、モデルは全体的に良好な結果を示していますが、いくつかのクラス（例: Ameri-can Bulldog, American Pit Bull Terrier, British Shorthair）で誤分類が見られます。パフォーマンス改善のために以下のプログラム改造を検討してください：

1. **データ拡張の強化**  
   - `transforms.Compose`にランダム回転、反転、明るさ調整などのデータ拡張を追加して、モデルの汎化性能を向上させます。例: `transforms.RandomHorizontalFlip()`, `trans-forms.RandomRotation(10)`。

2. **学習率の調整**  
   - 現在の学習率（`lr=2e-5`）が低すぎる可能性があります。初期学習率を`5e-5`や`1e-4`に増やし、学習率スケジューラ（例: `torch.optim.lr_scheduler.ReduceLROnPlateau`）を追加して収束を最適化します。

3. **バッチサイズの増加**  
   - バッチサイズを8から16や32に増やして、勾配推定の安定性を改善。ただし、メモリ制約に注意してください。

4. **モデルのアーキテクチャ改善**  
   - `coatnet_0_rw_224`からより深いモデル（例: `coatnet_1_rw_224`や`coatnet_2_rw_224`）に変更し、特徴抽出能力を強化。過学習に注意しつつ試してください。

5. **クラス不均衡の補正**  
   - 混同行列で一部クラス（例: British Shorthair）の誤分類が多い場合、クラスウェイトを`CrossEntropyLoss`に適用（例: `criterion = CrossEntropyLoss(weight=class_weights)`）してバランスを調整。

6. **早期終了（Early Stopping）**  
   - 検証損失が一定エポック（例: 3エポック）改善しない場合に学習を停止するロジックを追加して、過学習を防ぎます。

D先生： “どうです？”

QEU:FOUNDER ： “これを使えばPDCAがきちんと回るんじゃないんですか？まあ、これ以上はやる必要はないです。今回は、まだ助走（ウォーミング）ですから・・・。”

D先生： “そういえば、トライアルの途中で、こんなメッセージ（↓）が出てきました。”

### ✅ 4. アテンションマップの可視化（`attention_map_coatnet.py`）

**> CoAtNetは通常、Attentionマップを直接出力しないが、`timm`で構築された場合、**中間層の特徴マップ**や**Grad-CAM**で代替的に可視化可能です。**

### Grad-CAM を使用した可視化例：

QEU:FOUNDER  ： “あれ？COATNETは、アテンションマップを作画できないの？”

![imageVATT2-3-8](/2025-08-31-QEUR23_VATT3/imageVATT2-3-8.jpg) 

D先生： “とりあえず、Grad-CAM でやってみるしかないですね。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “AIを使うとバカになるっていうのは、そうなのかもね。・・・でも、バカには使いこなせないです。今回の件で、以下の用語(Term)を知らないとどうなったと思う？”

- アテンションマップ（レイヤー毎）
- ファインチューニング（レイヤーを限定した学習）
- 過学習（モデルが大きく、学習データが小さい場合）

C部長 : “手も足も出ないでしょうね。”

[![MOVIE1](http://img.youtube.com/vi/sgD0UKYQC6Y/0.jpg)](http://www.youtube.com/watch?v=sgD0UKYQC6Y "ALL in AI: 為什麼你要立即開始Vibe Coding?")

QEU:FOUNDER ： “それにしても、我々はトライ・エラーでこれらの知識を見につけたが、これから先はどうなるのかねえ・・・。今後は、若い人たちは仕事を始めてから結果に追われて、学習する時間を与えられないから・・・。ともあれ、「なにかをやりたい（目的）」を持っていれば、いづれは何とかなると思うよ。きっと・・・。”

![imageVATT2-3-9](/2025-08-31-QEUR23_VATT3/imageVATT2-3-9.jpg) 

QEU:FOUNDER ： “**Vibe Codingって、例えれば迷路づくり**みたいなものだから。よく、YouTube で「Vibe Codingがすごい！！」っていう話題で、アングリー・バードゲームを作るデモなどを見るよね。あれって、全然すごくないから・・・。やさしいメイズづくりみたいなものです。”

C部長 : “「メイズ（アングリー・バード）をつくる」っていう目的が達成されているだけですから。”

QEU:FOUNDER ： “**メイズの出来栄えで重要なのは、複雑なおもしろいメイズを作るための「制約条件」の付与の方なんです**。これが曲者でね・・・。小生がPer何某を好きじゃないのは、この部分なの・・・。”

C部長 : “だめなんですか・・・？”

QEU:FOUNDER ： “無料のプレイグランドを使っているから、本当のところは分かりません。それにしても、Per何某は小生が積み上げていった制約条件を簡単に変えちゃうんだよね。その観点からいうと、QWEN-CodeとGROKはいいです。それなりに尊重してくれます。しいて言うと、GROKは、プロジェクトの最初に使うと、気の利いた「たたき台プログラム」を作るのがうまいし、QWENはプログラムの改造が得意です。”

